\documentclass[a4paper,doc,natbib]{apa6}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\title{Research proposal: Output correctness guarantees with LLM guardrails}
\shorttitle{Output correctness guarantees with LLM guardrails}
\author{Yasin G\"{u}l \and Arjan Broer}
\affiliation{Open University of the Netherlands}

\abstract{
    Large Language Models (LLMs) demonstrate impressive performance in a wide range of tasks, yet their outputs are not always correct or reliable. This introduces challenges in applications where factual or logical correctness is critical. This research investigates whether guardrails—mechanisms designed to constrain or verify model output—can provide guarantees for output correctness. Using a design research approach, we will iteratively develop and evaluate such guardrails, focusing on how correctness can be defined, implemented, and assessed. The study aims to identify what types of guarantees are feasible and what trade-offs are involved in enforcing them.
}

\begin{document}

    \maketitle

    \section{Background}

    LLMs are known to produce coherent and contextually appropriate responses, yet they occasionally generate hallucinations, factual errors, or unsafe content. As discussed by \citet{dong2024safeguarding} and \citet{ayyamperumal2024current}, guardrails have emerged as a promising strategy to enhance the reliability of LLM outputs. These include rule-based filters, prompt constraints, and post-generation validation layers.

    While these approaches have shown improvements in output quality, they typically provide empirical gains rather than formal guarantees. This proposal takes a design research perspective to explore whether and how guardrails can be systematically constructed to offer stronger forms of output correctness assurance.

    \section{Research questions}

    This research explores the design and evaluation of guardrails that aim to ensure the correctness of outputs produced by LLMs. The central questions guiding this work are:

    \begin{description}
        \item[RQ1.] Can a guardrail be designed to ensure that the output generated by a large language model is 100\% correct?
        \item[RQ2.] How can the correctness of the output be operationalized and evaluated?
        \item[RQ3.] What design principles and mechanisms contribute to effective output guardrails?
    \end{description}

    \section{Research methods}

    This project will follow a design research methodology, consisting of iterative cycles of building, testing, and refining artifacts. The artifact in this case is a guardrail mechanism for LLMs that constrains or validates generated output.

    The evaluation focuses on whether the guardrail meets defined correctness criteria in realistic scenarios and what design choices contribute to its effectiveness.

    \subsection{Apparatus}

    All implementations will be written in Python, using APIs for open-access LLMs such as GPT-4-mini (OpenAI). The code will run on standard computing hardware.

    \subsection{Stimuli}

    To evaluate the guardrails, we will create or select representative tasks where output correctness is important (e.g., fact-based question answering, structured text generation, instructional content). These tasks will not be domain-specific but will represent typical LLM use cases with identifiable correctness constraints.

    \subsection{Design}

    This research uses an iterative design approach:
    \begin{itemize}
        \item \textbf{Cycle 1:} Define initial correctness criteria and implement a first prototype of a guardrail (e.g., regular expression filter, keyword validator).
        \item \textbf{Cycle 2:} Test the guardrail with generated outputs across several prompt types.
        \item \textbf{Cycle 3:} Refine the design based on observed failure modes, and repeat evaluation.
    \end{itemize}

    Each cycle informs the next, with adjustments based on both qualitative and quantitative feedback from the evaluation phase.

    \subsection{Procedure}

    \begin{enumerate}
        \item Identify output correctness criteria for selected tasks.
        \item Implement a prototype guardrail mechanism to enforce or check correctness.
        \item Generate model outputs for selected prompts with and without guardrails.
        \item Compare outputs against correctness criteria using human annotation or automated tools.
        \item Analyze failures, improve the design, and repeat.
    \end{enumerate}

    \subsection{Data analysis}

    The main analysis will track how each guardrail version performs across cycles:
    \begin{itemize}
        \item Quantitative metrics: error rate, number of violations, precision/recall if applicable.
        \item Qualitative analysis: types of failures not caught, limitations of the design.
        \item Reflective analysis: documentation of design choices and their impact.
    \end{itemize}

    \begin{description}
        \item[RQ1.] We assess whether any design iteration achieves 100\% correctness on the task set.
        \item[RQ2.] We evaluate different methods to define correctness (e.g., schema conformity, factuality checks) and their practical applicability.
        \item[RQ3.] We extract and summarize design principles from successful guardrail variants.
    \end{description}

    \section{Time schedule}

    The research will be conducted as part of a course. The proposal for the research will be submitted on 30 April 2025. A peer review of the proposal will be conducted by a fellow student. Feedback on the proposal will be provided shortly after 15 May 2025. The final report will be submitted on 15 July 2025.

    \bibliography{literature}

\end{document}
