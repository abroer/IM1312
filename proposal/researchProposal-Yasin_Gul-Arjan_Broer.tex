\documentclass[a4paper,doc,natbib]{apa6}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\title{Research proposal: Output correctness guarantees with LLM guardrails}
\shorttitle{Output correctness guarantees with LLM guardrails}
\author{Yasin G\"{u}l \and Arjan Broer}
\affiliation{Open University of the Netherlands}

\abstract{
    ToDo: Introduction to the problem statememt
    ToDo: Short summary of the research proposal
}
\begin{document}

    \maketitle

    \section{Background}

    Availalbe literature on guardrails for output correctness. Scan \cite{dong2024safeguarding} and \cite{ayyamperumal2024current} to find reference.

    \section{Research questions}

    Large Language models are powerful tools that can be used to generate text, but they are not always reliable.
    They can produce incorrect or misleading information, which can have serious consequences in certain applications.
    To address this issue, researchers have been exploring the use of guardrails to ensure the correctness of the output generated by these models.
    Guardrails are mechanisms that help to prevent the model from producing incorrect or harmful information.
    They can take many forms, including constraints on the input or output, additional training data, or post-processing steps.
    This research propsal will focus on guardrails for output correctness, which are designed to ensure that the output generated by the model is accurate and reliable.
    The main research question is whether it is possible to produce a guardrail that will be guaranteed to prevent some particular problem ever occurring with 100\% reliability.

    \begin{description}
        \item[RQ1.] Can a guardrail be designed to ensure that the output generated by a large language model is 100\% correct?
        \item[RQ2.] How can the correctness of the output be proven?
        \item[RQ3.] How can criteria for correctness of output be defined?
    \end{description}


    \section{Research methods}

    Find methods for testing and validating output guardrails in \cite{zhu2024testing}

    The experiment will be run as part of a course, meaning that no approval will be required from the local ethics committee, provided that data cannot be traced back to individual participants, participants are not expected to be harmed by taking part, participants take part voluntarily, and the data are only used for teaching purposes.

    \subsection{Apparatus}

    Any experimental guardrails will be implemented using Python and the GPT-o4-mini model as provided by Open AI.
    The software will be run on a home laptop, specialized hardware for the research is not required, nor available.

    \subsection{Stimuli}

    The experiment will be tested in the context of answering questions in the medical domain.
    The medical domain has clear documentation and taxonomy for symptoms, diseases and treatments, which makes it easier to define correctness criteria.
    The guardrails will be tested using a set of questions that are relevant to the medical domain, and the output generated by the model will be compared to the expected output.

    \subsection{Design}

    No idea on the design yet, but the open ai cookbook \cite{openai2023guardrails} has a lot of examples of how to use the model for different tasks.

    \subsection{Procedure}

    Describes what we do to answer the research questions and analyse the data we collect.

    \subsection{Data analysis}

    What does the data tell us?

    \begin{description}
        \item[RQ1. Can a guardrail be designed to ensure that the output generated by a large language model is 100\% correct?] <analysis result>
        \item[RQ2. How can the correctness of the output be proven?] <analysis result>
        \item[RQ3. How can criteria for correctness of output be defined?] <analysis result>
    \end{description}

    \section{Time schedule}

    The research will be conducted as part of a course. The course deadline dates are set by the university and the tasks need to be planned accordingly.
    The proposal for the research will be submitted on the 30 April 2025. A peer review of the proposal will be conducted by a fellow student.
    Feedback on the proposal will be provided by the course instructor short after the 15 May 2025.
    The final report will be submitted on the 15 July 2025.

    \bibliography{literature}

\end{document}




