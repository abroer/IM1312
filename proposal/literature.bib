@article{ayyamperumal2024current,
  title={Current state of LLM Risks and AI Guardrails},
  author={Ayyamperumal, Suriya Ganesh and Ge, Limin},
  journal={arXiv preprint arXiv:2406.12934},
  year={2024}
}
@article{dong2024safeguarding,
  title={Safeguarding large language models: A survey},
  author={Dong, Yi and Mu, Ronghui and Zhang, Yanghao and Sun, Siqi and Zhang, Tianle and Wu, Changshun and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Meng, Jie and others},
  journal={arXiv preprint arXiv:2406.02622},
  year={2024}
}
@article{zhu2024testing,
  title={Testing and Validation of a Custom Trained Large Language Model for HN Patients with Guardrails},
  author={Zhu, L and Anand, A and Gevorkyan, G and McGee, LA and Rwigema, JC and Rong, Y and Patel, SH},
  journal={International Journal of Radiation Oncology, Biology, Physics},
  volume={118},
  number={5},
  pages={e52--e53},
  year={2024},
  publisher={Elsevier}
}
@online{openai2023guardrails,
  author = {Colin Jarvis},
  title = {How to implement LLM guardrails},
  year = {2023},
  date = {19-12-2023},
  url = {https://cookbook.openai.com/examples/how_to_use_guardrails}
}