{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation of Automated Recipe Review Models\n",
    "\n",
    "This notebook analyzes the performance of different automated and manual review approaches on generated medical recipes.\n",
    "We will load the results, calculate all relevant metrics, and visualize the overlap in errors between the models.\n",
    "\n",
    "---"
   ],
   "id": "27a973d0809906ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn3\n",
    "import numpy as np\n",
    "\n",
    "# Show all columns in output\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "id": "a8c18a8bb1065978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We load the manually reviewed and automatically reviewed recipe datasets.\n",
    "Each file contains the field `medische_vraag` (medical question) and a `correct` boolean indicating whether the answer was judged as correct.\n"
   ],
   "id": "f569bb1774d8afd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "manual = pd.read_json('../data/generated_recipes_manual_review.json', dtype={'correct': 'boolean'})\n",
    "rag = pd.read_json('../data/generated_recipes_rag_review.json', dtype={'correct': 'boolean'})\n",
    "moderator = pd.read_json('../data/generated_recipes_moderator_review.json', dtype={'correct': 'boolean'})\n",
    "combined = pd.read_json('../data/generated_recipes_combined_review.json', dtype={'correct': 'boolean'})\n",
    "\n",
    "# Consistent column names for different models\n",
    "rag.rename(columns={'correct': 'correct_rag'}, inplace=True)\n",
    "moderator.rename(columns={'correct': 'correct_moderator'}, inplace=True)\n",
    "combined.rename(columns={'correct': 'correct_combined'}, inplace=True)\n"
   ],
   "id": "7881ebbb1e0d13db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Merge All Results\n",
    "\n",
    "We combine all results into a single DataFrame, using the medical question as the unique identifier.\n"
   ],
   "id": "7764c1ddabb406ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge on 'medische_vraag'\n",
    "total = manual.merge(rag[['medische_vraag', 'correct_rag']], on='medische_vraag', how='left')\n",
    "total = total.merge(moderator[['medische_vraag', 'correct_moderator']], on='medische_vraag', how='left')\n",
    "total = total.merge(combined[['medische_vraag', 'correct_combined']], on='medische_vraag', how='left')\n",
    "\n",
    "total.head()\n"
   ],
   "id": "45071e350dff6384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Compute Performance Metrics\n",
    "\n",
    "We calculate the confusion matrix (TP, FP, TN, FN) for each model, as well as precision, recall, accuracy, F1-score, and error rate.\n"
   ],
   "id": "c25bbcf451bcdcd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calc_confusion(true_col, pred_col):\n",
    "    tp = ((total[true_col] == True) & (total[pred_col] == True)).sum()\n",
    "    tn = ((total[true_col] == False) & (total[pred_col] == False)).sum()\n",
    "    fp = ((total[true_col] == False) & (total[pred_col] == True)).sum()\n",
    "    fn = ((total[true_col] == True) & (total[pred_col] == False)).sum()\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def calc_metrics(tp, tn, fp, fn):\n",
    "    total_count = tp + tn + fp + fn\n",
    "    error_rate = (fp + fn) / total_count if total_count else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    accuracy = (tp + tn) / total_count if total_count else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    return dict(\n",
    "        error_rate=error_rate,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        accuracy=accuracy,\n",
    "        f1=f1\n",
    "    )\n",
    "\n",
    "# Calculate for each model\n",
    "tp_rag, tn_rag, fp_rag, fn_rag = calc_confusion('correct', 'correct_rag')\n",
    "tp_mod, tn_mod, fp_mod, fn_mod = calc_confusion('correct', 'correct_moderator')\n",
    "tp_comb, tn_comb, fp_comb, fn_comb = calc_confusion('correct', 'correct_combined')\n",
    "\n",
    "metrics = {\n",
    "    'RAG': calc_metrics(tp_rag, tn_rag, fp_rag, fn_rag),\n",
    "    'Moderator': calc_metrics(tp_mod, tn_mod, fp_mod, fn_mod),\n",
    "    'Combined': calc_metrics(tp_comb, tn_comb, fp_comb, fn_comb)\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "pd.DataFrame(metrics).T\n"
   ],
   "id": "b177d47d0deea027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Visualize Confusion Matrices\n",
    "\n",
    "Let's plot the confusion matrices for each model.\n"
   ],
   "id": "fc611ec26d3496b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(tp, tn, fp, fn, label):\n",
    "    matrix = np.array([[tp, fn],\n",
    "                       [fp, tn]])\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Correct', 'Predicted Incorrect'],\n",
    "                yticklabels=['Actual Correct', 'Actual Incorrect'])\n",
    "    plt.title(f\"Confusion Matrix - {label}\")\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"True Value\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(tp_rag, tn_rag, fp_rag, fn_rag, \"RAG\")\n",
    "plot_confusion_matrix(tp_mod, tn_mod, fp_mod, fn_mod, \"Moderator\")\n",
    "plot_confusion_matrix(tp_comb, tn_comb, fp_comb, fn_comb, \"Combined\")\n"
   ],
   "id": "e77793f34fd05360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Bar Charts: Model Comparison\n",
    "\n",
    "Stacked and grouped bar plots for model comparison of TP, FP, TN, FN, and overall metrics.\n"
   ],
   "id": "e084ddd346a6ac53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = ['RAG', 'Moderator', 'Combined']\n",
    "tp_list = [tp_rag, tp_mod, tp_comb]\n",
    "tn_list = [tn_rag, tn_mod, tn_comb]\n",
    "fp_list = [fp_rag, fp_mod, fp_comb]\n",
    "fn_list = [fn_rag, fn_mod, fn_comb]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot each segment and store the bar containers\n",
    "tp_bar = plt.bar(x, tp_list, color='green', label='True Positives (TP)')\n",
    "tn_bar = plt.bar(x, tn_list, bottom=tp_list, color='lightgreen', label='True Negatives (TN)')\n",
    "fp_bottom = [tp + tn for tp, tn in zip(tp_list, tn_list)]\n",
    "fp_bar = plt.bar(x, fp_list, bottom=fp_bottom, color='red', label='False Positives (FP)')\n",
    "fn_bottom = [tp + tn + fp for tp, tn, fp in zip(tp_list, tn_list, fp_list)]\n",
    "fn_bar = plt.bar(x, fn_list, bottom=fn_bottom, color='darkred', label='False Negatives (FN)')\n",
    "\n",
    "# --- Annotate each bar segment with its value ---\n",
    "for i in range(len(methods)):\n",
    "    plt.text(x[i], tp_list[i] / 2, str(tp_list[i]), ha='center', va='center', color='white', fontsize=9)\n",
    "    plt.text(x[i], tp_list[i] + tn_list[i] / 2, str(tn_list[i]), ha='center', va='center', color='black', fontsize=9)\n",
    "    plt.text(x[i], fp_bottom[i] + fp_list[i] / 2, str(fp_list[i]), ha='center', va='center', color='white', fontsize=9)\n",
    "    plt.text(x[i], fn_bottom[i] + fn_list[i] / 2, str(fn_list[i]), ha='center', va='center', color='white', fontsize=9)\n",
    "\n",
    "# --- Finalize plot ---\n",
    "plt.xticks(x, methods)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Performance per Approach (TP/TN/FP/FN)')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Vertical grouped bar chart: Metrics per model, grouped by metric ---\n",
    "\n",
    "def safe_div(numerator, denominator):\n",
    "    return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "# Calculate metrics for each model\n",
    "precisions = [safe_div(tp, tp + fp) for tp, fp in zip(tp_list, fp_list)]\n",
    "recalls = [safe_div(tp, tp + fn) for tp, fn in zip(tp_list, fn_list)]\n",
    "f1_scores = [safe_div(2 * p * r, p + r) if (p + r) > 0 else 0 for p, r in zip(precisions, recalls)]\n",
    "accuracies = [safe_div(tp + tn, tp + tn + fp + fn) for tp, tn, fp, fn in zip(tp_list, tn_list, fp_list, fn_list)]\n",
    "\n",
    "metric_labels = ['Precision', 'Recall', 'F1 Score', 'Accuracy']\n",
    "metrics_per_model = [precisions, recalls, f1_scores, accuracies]\n",
    "colors = ['blue', 'green', 'red']\n",
    "bar_width = 0.2\n",
    "x_metrics = np.arange(len(metric_labels))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    metric_values = [metric[i] for metric in metrics_per_model]\n",
    "    plt.bar(x_metrics + i * bar_width, metric_values, bar_width, label=method, color=color)\n",
    "    for idx, value in enumerate(metric_values):\n",
    "        plt.text(x_metrics[idx] + i * bar_width, value + 0.01, f\"{value*100:.1f}%\", ha='center', va='bottom', color='black', fontsize=9)\n",
    "\n",
    "plt.xticks(x_metrics + bar_width, metric_labels)\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.title('Model Performance Comparison per Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d3856db01b32bc02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Error Overlap Analysis\n",
    "\n",
    "We analyze which medical questions are incorrectly answered (FP/FN) by which models.\n",
    "This helps to understand where the models make unique or shared mistakes.\n"
   ],
   "id": "414f88af40739760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fp_rag_set = set(total[(total['correct'] == False) & (total['correct_rag'] == True)]['medische_vraag'])\n",
    "fp_mod_set = set(total[(total['correct'] == False) & (total['correct_moderator'] == True)]['medische_vraag'])\n",
    "fp_comb_set = set(total[(total['correct'] == False) & (total['correct_combined'] == True)]['medische_vraag'])\n",
    "\n",
    "fn_rag_set = set(total[(total['correct'] == True) & (total['correct_rag'] == False)]['medische_vraag'])\n",
    "fn_mod_set = set(total[(total['correct'] == True) & (total['correct_moderator'] == False)]['medische_vraag'])\n",
    "fn_comb_set = set(total[(total['correct'] == True) & (total['correct_combined'] == False)]['medische_vraag'])\n",
    "\n",
    "# Print overlap summary\n",
    "print(\"False Positives (FP) Overlap Summary:\")\n",
    "print({\n",
    "    \"FP unique to RAG\": len(fp_rag_set - fp_mod_set - fp_comb_set),\n",
    "    \"FP unique to Moderator\": len(fp_mod_set - fp_rag_set - fp_comb_set),\n",
    "    \"FP unique to Combined\": len(fp_comb_set - fp_rag_set - fp_mod_set),\n",
    "    \"FP overlap (all)\": len(fp_rag_set & fp_mod_set & fp_comb_set)\n",
    "})\n",
    "\n",
    "print(\"False Negatives (FN) Overlap Summary:\")\n",
    "print({\n",
    "    \"FN unique to RAG\": len(fn_rag_set - fn_mod_set - fn_comb_set),\n",
    "    \"FN unique to Moderator\": len(fn_mod_set - fn_rag_set - fn_comb_set),\n",
    "    \"FN unique to Combined\": len(fn_comb_set - fn_rag_set - fn_mod_set),\n",
    "    \"FN overlap (all)\": len(fn_rag_set & fn_mod_set & fn_comb_set)\n",
    "})\n",
    "\n",
    "def print_question_set(label, s):\n",
    "    \"\"\"Print label and set/list, with 'empty' if empty.\"\"\"\n",
    "    if not s:\n",
    "        print(f\"  {label}: empty\")\n",
    "    else:\n",
    "        print(f\"  {label}:\")\n",
    "        for v in sorted(s):\n",
    "            print(f\"    - {v}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"False Positives (FP) - Detailed Questions\")\n",
    "print(\"=\"*50)\n",
    "print_question_set(\"Unique to RAG\", fp_rag_set - fp_mod_set - fp_comb_set)\n",
    "print_question_set(\"Unique to Moderator\", fp_mod_set - fp_rag_set - fp_comb_set)\n",
    "print_question_set(\"Unique to Combined\", fp_comb_set - fp_rag_set - fp_mod_set)\n",
    "print_question_set(\"Overlap in all models\", fp_rag_set & fp_mod_set & fp_comb_set)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"False Negatives (FN) - Detailed Questions\")\n",
    "print(\"=\"*50)\n",
    "print_question_set(\"Unique to RAG\", fn_rag_set - fn_mod_set - fn_comb_set)\n",
    "print_question_set(\"Unique to Moderator\", fn_mod_set - fn_rag_set - fn_comb_set)\n",
    "print_question_set(\"Unique to Combined\", fn_comb_set - fn_rag_set - fn_mod_set)\n",
    "print_question_set(\"Overlap in all models\", fn_rag_set & fn_mod_set & fn_comb_set)\n"
   ],
   "id": "c6355491355f3c56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Venn Diagrams of Overlapping Errors\n",
    "\n",
    "Visualize the overlap of FP and FN errors between models with Venn diagrams.\n"
   ],
   "id": "bb8733e0fdc533c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Venn Diagram of False Positives\")\n",
    "venn3([fp_rag_set, fp_mod_set, fp_comb_set], set_labels=(\"RAG\", \"Moderator\", \"Combined\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Venn Diagram of False Negatives\")\n",
    "venn3([fn_rag_set, fn_mod_set, fn_comb_set], set_labels=(\"RAG\", \"Moderator\", \"Combined\"))\n",
    "plt.show()\n"
   ],
   "id": "3273da5e5db7d338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "- **Moderator** achieves the highest recall (95.0%), meaning it correctly identifies almost all true positives. However, this comes at the cost of precision (72.7%), so it produces more false positives compared to the other models. The Moderator also has the highest F1-score (82.4%) and solid accuracy (71.4%).\n",
    "\n",
    "- **RAG** is balanced, with precision, recall, and F1-score all at 77.9%. Its accuracy is slightly lower (68.8%), reflecting that it is neither overly cautious nor overly aggressive, but more \"conservative\" than Moderator in terms of false positives.\n",
    "\n",
    "- **Combined** has the highest precision (78.8%), but its recall (74.3%) and F1-score (76.5%) are slightly lower than Moderator. Its accuracy (67.8%) is also the lowest of the three, indicating that combining models does not necessarily produce better overall results for this dataset.\n",
    "\n",
    "**Summary:**\n",
    "No single approach dominates on all metrics.\n",
    "- If you value catching every correct answer (high recall), **Moderator** is best, but expect more false positives.\n",
    "- If you want balanced performance, **RAG** is a safe choice.\n",
    "- If you prefer to minimize false positives, **Combined** offers the highest precision, but with more missed positives."
   ],
   "id": "32af3b4ae866ac3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
