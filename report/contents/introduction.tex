% From the Research Methods workbook, page 241.
%
% In technical papers, the introduction presents the
% general background and the research questions. A detailed
% literature review is provided in the next section, called ‘Related
% work’. In the domain of psychology, which is more closely related
% to the study of how humans interact with AI, the introduction will
% contain the general background, literature review and research
% questions. There is some debate about whether to specify the
% research questions as actual questions, or to use phrases such as
% ‘This research will determine whether...’.

\subsection{Background}

Large Language Models (LLMs) are capable of generating coherent and contextually appropriate responses.
However, they occasionally produce hallucinations, factual inaccuracies, or unsafe advice.
This risk becomes particularly critical in medical contexts, where incorrect information can have serious consequences.
Recent research \citep{waldock2025curriculum} shows that LLMs can generate incorrect answers on medical prompts and students (or professionals) are required to valide the correctness of the output.
The study shows an accuracy rate of 50\% In this case the student acted as a human gaurdrail on the output of the LLM.

Examples include suggesting dangerous dosages, recommending inappropriate medications, or failing to warn against contraindications \citep{luo2024clinical}, \citep{schieszer2023large}.
For instance, providing a baby with an adult dose of ibuprofen, prescribing antibiotics for viral infections, or advising unsafe use of controlled substances are all examples where hallucinated outputs could cause harm.

A performance review of ChatGPT on the United States Medical Licensing Examination (USMLE) showed that the model performed at a level comparable to human physicians, but with a significant number of incorrect answers \citep{kung2023performance}.
This shows that the model can be expected to produce incorrect answers, even when it is capable of generating correct ones.

Guardrails have emerged as a strategy to mitigate these risks by constraining or verifying model outputs.
Techniques such as retrieval-augmented generation (RAG) and output moderation models have been proposed to systematically improve output reliability \citep{dong2024guardrails}.
Retrieval-augmented generation grounds answers in external, verified sources, reducing hallucinations, while moderator models can filter unsafe or incorrect content after generation \citep{inan2023llamaguard}.

In the medical domain, ensuring correct advice is even more crucial, particularly regarding medications.
Recent work shows that LLMs can contribute to reducing medication instruction errors when proper safeguards are in place \citep{pais2024medication}.

This proposal explores whether guardrails can be systematically designed and combined to guarantee correctness in LLM-generated answers for medication-related medical prompts.
We particularly investigate the effectiveness of retrieval-augmented generation and moderator models, individually and in combination, to prevent incorrect or unsafe medical advice.

\subsection{Related Work}
Interesting article in \ref{kang2024r} about guardrails based on probabilistic reasoning.

\subsection{Research Questions}
This research explores the design and evaluation of guardrails that aim to ensure the correctness of outputs produced by LLMs for medication-related medical queries.
The central questions guiding this work are:

\begin{description}
    \item[RQ1.] Can a guardrail mechanism combining RAG and moderator models ensure 100\% correctness in LLM responses to medical prompts?
    \item[RQ2.] How can correctness be operationalized for medication-related queries (e.g., dosage, indications, contraindications)?
    \item[RQ3.] What are the strengths and limitations of using RAG and moderator models in ensuring factual correctness in the medical domain?
\end{description}

\subsection{Null hypothesis}
The null hypothesis for this research is that the guardrail mechanisms (RAG, moderator model, and their combination)
do not significantly improve the correctness of LLM outputs compared to a baseline without guardrails.
In the below research methods section, a baseline condition is used to represent the LMM without guardrails.
