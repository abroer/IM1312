% From the Research Methods workbook, page 242.
%
%% This section explains how the research was performed.
% Sometimes it has a different name (e.g., ‘Experiments’), but still has
% the same purpose: to describe the methods. The section should
% allow another researcher to replicate the study (i.e., perform it
% again). This is different from obtaining the same findings). In
% psychology, this section has subsections describing participants,
% design, stimuli, procedure, and data analysis.

This project will follow a design research methodology \citep{wieringa2014design}, consisting of iterative cycles of building, testing, and refining artifacts.
The artifact in this case is a guardrail mechanism for LLMs that constrains or validates generated output.

We will test and compare four experimental conditions to evaluate the effectiveness of different guardrail approaches for ensuring correctness in medical prompts:

\begin{itemize}
    \item \textbf{Baseline:} No guardrails applied.
    \item \textbf{RAG-only:} Responses are grounded in external verified medical sources using retrieval-augmented generation \citep{dong2024guardrails}.
    \item \textbf{Moderator-only:} A second AI model evaluates the generated output and flags or blocks incorrect or unsafe content \citep{inan2023llamaguard}.
    \item \textbf{Combined:} Both RAG and moderator-AI are applied sequentially, checking it with both methods.
\end{itemize}

The evaluation focuses on whether the guardrail meets defined correctness criteria in realistic scenarios and what design choices contribute to its effectiveness.

\subsection{Baseline setup}
In the baseline setup (\autoref{fig:baselineSetup}) we will use a large language model (LLM) to generate responses to medical prompts without any guardrails.
The model used is the Google Gemini 2.0 flash model, which is a large language model trained on a wide range of data, including medical texts \citep{saab2024capabilities}.
The medical question will be directly used as prompt for the LLM.
Later iterations of the project proved that the medical question needed to be extended with additional patient information.
The information was primarily needed for manual review by an expert user as the medical context of the patient is important to validate if the recipe is correct.
Together with the prompt the LLM wil be provided with a system instruction that specifies the task and expected output format.
As the medical question if formulated in Dutch, the system instruction is also in Dutch.
\begin{verbatim}
    Je bent een medisch expert. Beantwoord de medische vraag met
    een recept volgens het volgende template:
    'Rp. [Medicijnaam] [Sterkte] [Vorm]\ndtd. No. [Aantal]\n S. [Instructie]'.
    Neem relevante informatie over de patiënt in acht, zoals leeftijd en
    eventuele allergieën.
    Geef de formele medicijnnaam in het JSON-veld 'medicijn'.
    Benoem de categorie van het medicijn in het JSON-veld 'categorie'.
    Geef bij de instructie ook de duur van de behandeling aan.
    Herhaal de medische vraag in het JSON-veld 'medische_vraag'.
    Voeg de patiëntinformatie toe in het JSON-veld 'patientinformatie'.
\end{verbatim}

\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{figures/baselineSetup.png}
    \captionof{figure}{Baseline setup for generating medical recipes without guardrails.}
    \label{fig:baselineSetup}
\end{figure}

\subsection{RAG-only setup}

In the RAG-only setup (\autoref{fig:ragSetup}), we will use a large language model (LLM) to generate responses to medical prompts and check the response using retrieval-augmented generation (RAG).
The RAG component will retrieve relevant information about the medication from apotheek.nl and use that information to validate the generated output.
The output will be validated for correctness of medicine, dosage, shape, frequency and instruction taking the context of the patient information into consideration.
When the output is not correct the response will be reported as false.
This allows the system to take corrective actions when the response is incorrect.

\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{figures/RAGSetup.png}
    \captionof{figure}{RAG-only setup for generating medical recipes with retrieval-augmented generation.}
    \label{fig:ragSetup}
\end{figure}

\subsection{Moderator-only setup}

In the moderator-only setup (\autoref{fig:moderatorSetup}), we will use a large language model (LLM) to generate responses to medical prompts and check the response with a moderator LLM model.
The moderator model will evaluate the generated output and flag or block incorrect or unsafe content.
The same model will be used for both the generation and moderation tasks, but with different system instructions.

\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{figures/moderatorSetup.png}
    \captionof{figure}{Moderator-only setup for generating medical recipes with a moderator model.}
    \label{fig:moderatorSetup}
\end{figure}

\subsection{Combined setup}

In the combined setup (\autoref{fig:combinedSetup}), we will use a large language model (LLM) to generate responses to medical prompts and check the response with both retrieval-augmented generation (RAG) and a moderator model.
The RAG component will retrieve relevant information about the medication from apotheek.nl and use that information to validate the generated output.
The moderator model will evaluate the generated output and flag or block incorrect or unsafe content.
The recipe is considered correct when it is both grounded in the retrieved information and passes the moderation check.

\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{figures/combinedSetup.png}
    \captionof{figure}{Combined setup for generating medical recipes with retrieval-augmented generation and a moderator model.}
    \label{fig:combinedSetup}
\end{figure}

\subsection{Apparatus}

The LLM used for the baseline, RAG-only, moderator-only, and combined setups will be the Google Gemini 2.0 flash model.
No changes have been made to the model parameters like temperature or top-p sampling, as the model is already optimized for generating high-quality responses.
To be able to process the entire dataset efficiently, the medical questions and recipes will be processed in the baseline setup as one big batch of 200 questions.
For running the guardrail mechanisms, the model will be called in a loop each baseline answer.
This setup deviates from the intended guardrail setup, but makes data analysis easier and more efficient.
When the initial prompt to the model would generate different answers the performance of the guardrail mechanisms would be harder to compare.
Feeding the same baseline answers to all guardrail mechanisms allows for a fair comparison of the performance of the different guardrail mechanisms.

\subsection{Stimuli}

To evaluate the guardrails, we will use a fixed set of prompts that represent medical questions including patient information.
These tasks represent real-world LLM use cases with identifiable correctness constraints \citep{pais2024medication}.
The responses will be formatted as formal medical recipes following the Dutch guidelines for medication prescriptions \citep{farmacotherapeutischkompas}.
\begin{verbatim}
        Rp.   [Medicijnnaam] [Sterkte] [Vorm]
          dtd. No. [Aantal]
          S. [Instructie]
\end{verbatim}

The RAG guardrail will first find the medication information on apotheek.nl using a Google search.
The relevant information will be extracted from the retrieved web page and feeded to the LLM as part of the prompt.
The moderator guardrail will use the same system instruction as the RAG guardrail, but will not use the retrieved information.

\subsection{Dataset}

The dataset consists of a set of 200 medical questions and relevant patient information including age, gender and symptoms.
The medical questions and patient information will be used as prompt input for the baseline setup to generate answers as recipes.
The recipes will be used by the guardrails to evaluate the correctness of the generated output.
Because patient information is subject to privacy regulations, the dataset is completely synthetic and has been generated by the researchers.
To make sure the dataset is representative of real-world medical questions all questions and patient information has been reviewed by a medical expert.

\subsection{Design}

Each condition will be tested across the same prompt set.
Performance will be evaluated in terms of correctness, safety, and response quality.
Correctness of the output will be validated against the available medication guidelines available in the ``Farmacotherapeutisch Kompas'' \citep{farmacotherapeutischkompas} and the ``Geneesmiddeleninformatiebank'' \citep{geneesmiddeleninformatiebank}.
The design is iterative, allowing refinements to each method across evaluation cycles.
Each cycle will test the correctness of a medical prompt based on datasets correct and incorrect medical recipes.
The same dataset will be used for all four conditions, ensuring a fair comparison.

\subsection{Procedure}

\begin{enumerate}
    \item Identify output correctness criteria for selected medical prompts.
    \item Implement guardrail mechanisms: RAG, moderator model, and their combination.
    \item For each prompt, generate model outputs in four conditions: Baseline, RAG-only, Moderator-only, and Combined.
    \item Compare outputs against correctness criteria using expert annotation or rule-based validation.
    \item Analyze failures, compare effectiveness, and refine guardrail designs.
\end{enumerate}

\subsection{Data analysis}

Each guardrail condition (Baseline, RAG-only, Moderator-only, Combined) will be evaluated using the following metrics:

\begin{itemize}
    \item \textbf{Error rate:} Percentage of incorrect or unsafe answers.
    \item \textbf{Precision/Recall:} For detecting unsafe or medically incorrect outputs.
    \item \textbf{False positives / negatives:} Analysis of over- or under-blocking.
    \item \textbf{Qualitative analysis:} Categorization of failure types per condition.
    \item \textbf{Reflective analysis:} Documentation of design choices and their impact.
\end{itemize}

This comparison allows us to determine whether a specific approach—or their combination—can achieve 100\% correctness or significantly reduce critical errors.
